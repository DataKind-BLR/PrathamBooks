{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../app/app.py\n",
    "import requests\n",
    "import urllib\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "API_URI = 'https://storyweaver.org.in/api/v1/'\n",
    "\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "def get_pages_info(resp):\n",
    "    '''Parse the request from the storyweaver api and get text of the story book\n",
    "    '''\n",
    "    pages = resp['data']['pages']\n",
    "    parsed_info = {'texts': [],\n",
    "                   'image_url': None,\n",
    "                   'title': None}\n",
    "    for page in pages:\n",
    "        if page['pageType'] == 'FrontCoverPage':\n",
    "            if parsed_info['image_url'] is None:\n",
    "                parsed_info['image_url'] = page['coverImage']['sizes'][1]['url']\n",
    "            if parsed_info['title'] is None:\n",
    "                soup = BeautifulSoup(page['html'])\n",
    "                title = soup.findAll(\"p\", {\"class\": \"cover_title\"})[0].text\n",
    "                parsed_info['title'] = title\n",
    "        if page['pageType'] == 'StoryPage':\n",
    "            cleantext = BeautifulSoup(page['html'], \"lxml\").text.replace('\\n', ' ').replace('  ','')\n",
    "            # remove unicode\n",
    "            cleantext = unicodedata.normalize('NFKC', cleantext).replace('\\\"', '')\n",
    "            parsed_info['texts'].append(cleantext)\n",
    "    parsed_info['text_str'] = ' '.join(parsed_info['texts'])\n",
    "    return parsed_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../scripts/freq_word_extractor.py\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def get_ngrams(tokens, n):\n",
    "    return [' '.join(list(words)) for words in list(ngrams(tokens, n))]\n",
    "\n",
    "\n",
    "def clean_and_tokenize_text(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    # remove punctuations, lower and tokenize the text\n",
    "    stripped = [w.translate(table) for w in text.lower().split()]\n",
    "    [STOP_WORDS.add(word) for word in ['said', 'says',\n",
    "                                       'saying', 'ask',\n",
    "                                       'asking', 'like',\n",
    "                                       'say']]\n",
    "    words = [word for word in stripped if word.isalpha() and word not in STOP_WORDS]\n",
    "    return words\n",
    "\n",
    "\n",
    "def get_best_keywords(text):\n",
    "    # pick top n based on distance from the max frequency\n",
    "    words_df = pd.DataFrame(get_top_k_n_words(text, 20), columns=['word', 'freq'])\n",
    "    words_df['normalized_freq'] = words_df.apply(lambda x: x.freq + len(x.word.split()), axis=1)\n",
    "    words_df['z_score'] = (words_df.normalized_freq - words_df.normalized_freq.mean()) / words_df.normalized_freq.std(ddof=0)\n",
    "    return list(words_df[words_df.z_score > 1].word.values)\n",
    "\n",
    "\n",
    "def get_top_k_n_words(text, k=5, n=2):\n",
    "    tokens = clean_and_tokenize_text(text)\n",
    "    ngrams = get_ngrams(tokens, n)\n",
    "    freq = Counter(tokens + ngrams)\n",
    "    return freq.most_common(k)\n",
    "\n",
    "\n",
    "def get_top_bigrams(text, n):\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(clean_and_tokenize_text(text))\n",
    "    finder.apply_freq_filter(2)\n",
    "    return [' '.join(list(words)) for words in finder.nbest(bigram_measures.raw_freq, n)]\n",
    "\n",
    "\n",
    "def get_freq_keywords(text):\n",
    "    collocations = get_top_bigrams(text, 5)\n",
    "    freq = [word for word, freq in get_top_k_n_words(text, 10)]\n",
    "    combined_tags = set(collocations + freq + get_best_keywords(text))\n",
    "    return [word for word in list(combined_tags) if len(word) > 3][:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_book_link(book_link):\n",
    "    link = API_URI + 'stories/{}/read'.format(book_link)\n",
    "    resp = requests.get(link).json()\n",
    "    parsed_resp = get_pages_info(resp)\n",
    "    text = ' '.join(parsed_resp['texts'])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract keyword from title\n",
    "    - Try Pos Tags\n",
    "- Extract keywords from text\n",
    "    - Co-occurence\n",
    "    - Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags for 28270-anna-s-extraordinary-experiments-with-weather\n",
      "['august anna', 'weather', 'anna mani', 'scientist', 'many', 'anna', 'books anna', 'books books', 'mani', 'birthday']\n",
      "tags for 34911-the-case-of-the-missing-water\n",
      "['pump man', 'finally sat', 'climbed tank', 'find', 'tank', 'know water', 'stream', 'school', 'ranj sapna', 'sapna']\n",
      "tags for 7-fat-king-thin-dog\n",
      "['thin', 'king thin', 'dog fat', 'thin dog', 'king', 'runs', 'run run', 'fat king', 'bird']\n",
      "tags for 44659-meera-and-ameera\n",
      "['everything likes', 'person', 'know', 'make', 'ameera', 'meera', 'favourite person', 'everything', 'favourite', 'world know']\n",
      "tags for 26690-miss-laya-s-fantastic-motorbike-does-not-like-fruits\n",
      "['fantastic motorbike', 'dhup', 'times', 'motorbike', 'pineapples', 'chandra', 'fantastic', 'laya', 'claps six', 'clapclap']\n"
     ]
    }
   ],
   "source": [
    "book_links = ['28270-anna-s-extraordinary-experiments-with-weather',\n",
    "              '34911-the-case-of-the-missing-water',\n",
    "              '7-fat-king-thin-dog',\n",
    "              '44659-meera-and-ameera',\n",
    "              '26690-miss-laya-s-fantastic-motorbike-does-not-like-fruits']\n",
    "\n",
    "for book_link in book_links:\n",
    "    text = get_text_from_book_link(book_link)  \n",
    "    collocations = get_top_bigrams(text, 5)\n",
    "    freq = [word for word, freq in get_top_k_n_words(text, 10)]\n",
    "    combined_tags = set(collocations + freq + get_best_keywords(text))\n",
    "    print('tags for {}'.format(book_link))\n",
    "    print([word for word in list(combined_tags) if len(word) > 3][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "??wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner",
   "language": "python",
   "name": "ner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
