{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from ordered_set import OrderedSet\n",
    "import numpy\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"content.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['story_id', 'story_title', 'story_english_title',\n",
       "       'is_child_created_story', 'stories_status', 'stories_summary',\n",
       "       'ancestry', 'is_recommended_story', 'reads', 'language_name',\n",
       "       'organization_name', 'page_type', 'story_derivation_type',\n",
       "       'story_publishing_type', 'reading_level_cat', 'story_content'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text=pd.DataFrame()\n",
    "data_text['story_content'] = data.loc[:,'story_content']\n",
    "data_text['index'] = data.loc[:,'story_id']\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhishekkumaryadav\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abhishekkumaryadav\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2046, 2)\n"
     ]
    }
   ],
   "source": [
    "df=documents\n",
    "#1314 contained korean\n",
    "df=df.drop(df.index[1314])\n",
    "#1311 contained nan value\n",
    "df=df.drop(df.index[1311])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying preprocess method to entire story_content\n",
    "processed_docs = list(map(preprocess,df.loc[0:2046,'story_content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos function return pos tagging for a list of words  and returns a list\n",
    "def pos(lis):\n",
    "    return nltk.pos_tag(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed_docs is a list with each element for each story and each element being a list of keywords\n",
    "processed_docsPOS=list(map(pos,processed_docs))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating noun and adjective pos tags from this list of tuples\n",
    "l1=['NN','NNS','NNP','NNPS']\n",
    "l2=['JJ','JJR','JJS']\n",
    "def sep(lis):\n",
    "    ansNoun=[]\n",
    "    ansAdj=[]\n",
    "    for i in lis:\n",
    "        if(i[1] in l1):\n",
    "            ansNoun.append(i[0])\n",
    "        if(i[1] in l2):\n",
    "            ansAdj.append(i[0])\n",
    "    return ansNoun,ansAdj\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docsPOSSep=list(map(sep,processed_docsPOS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating the Noun And Adjective keywords separately\n",
    "processed_docsPOSNoun=[]\n",
    "processed_docsPOSAdj=[]\n",
    "for i in processed_docsPOSSep:\n",
    "    processed_docsPOSNoun.append(i[0])\n",
    "    processed_docsPOSAdj.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc2vec  gives a vector for a document\n",
    "#word2vec gives a vector for a word\n",
    "#calculating the similarity score b/w word and document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here processed_docsPOSSep replace with the file downloaded from standford for training the model\n",
    "model1 = Word2Vec(processed_docsPOSAdj, min_count=1,size=50)\n",
    "model2 = Word2Vec(processed_docsPOSNoun, min_count=1,size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working with doc2vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents1 = [TaggedDocument(doc, [i]) for i, doc in enumerate(processed_docsPOSAdj)]\n",
    "model11 = Doc2Vec(documents1, vector_size=50, window=2, min_count=1, workers=4)\n",
    "documents2 = [TaggedDocument(doc, [i]) for i, doc in enumerate(processed_docsPOSNoun)]\n",
    "model12 = Doc2Vec(documents2, vector_size=50, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will apply MMD equation  S=λA- (1-λ) max sim(Wi,Wj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fixed λ\n",
    "# to 0.5 in the adapted MMR equation (2), to ensure\n",
    "# equal importance to informativeness and diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1 is model for doc2vec\n",
    "#model is model for word2vec\n",
    "STAdj=[]\n",
    "for i in processed_docsPOSAdj:\n",
    "    #vector for this document\n",
    "    vectorD=model11.infer_vector(i)\n",
    "    S=[]\n",
    "    for j in i:\n",
    "        #vector for this word\n",
    "        vectorW=model1.wv[j]\n",
    "        A=numpy.dot(vectorD,vectorW)\n",
    "#         maxSim=0\n",
    "#         for k in i:\n",
    "#             if(numpy.dot(model.wv[j],model.wv[k])>maxSim):\n",
    "#                 maxSim=numpy.dot(model.wv[j],model.wv[k])\n",
    "#         S.append(0.5*A-0.5*maxSim)\n",
    "        S.append((A,j))\n",
    "    STAdj.append(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1 is model for doc2vec\n",
    "#model is model for word2vec\n",
    "STNoun=[]\n",
    "for i in processed_docsPOSNoun:\n",
    "    #vector for this document\n",
    "    vectorD=model12.infer_vector(i)\n",
    "    S=[]\n",
    "    for j in i:\n",
    "        #vector for this word\n",
    "        vectorW=model2.wv[j]\n",
    "        A=numpy.dot(vectorD,vectorW)\n",
    "#         maxSim=0\n",
    "#         for k in i:\n",
    "#             if(numpy.dot(model.wv[j],model.wv[k])>maxSim):\n",
    "#                 maxSim=numpy.dot(model.wv[j],model.wv[k])\n",
    "#         S.append(0.5*A-0.5*maxSim)\n",
    "        S.append((A,j))\n",
    "    STNoun.append(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKey(item):\n",
    "    return item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting the keywords as per their similarity score\n",
    "STAdj1=[]\n",
    "for i in STAdj:\n",
    "    STAdj1.append(sorted(i,key=getKey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "STNoun1=[]\n",
    "for i in STNoun:\n",
    "    STNoun1.append(sorted(i,key=getKey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing the duplicates with just one of it's own kind\n",
    "STAdj11=[]\n",
    "for i in STAdj1:\n",
    "    STAdj11.append(list(OrderedSet(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "STNoun11=[]\n",
    "for i in STNoun1:\n",
    "    STNoun11.append(list(OrderedSet(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limiting the words in each document to 10 \n",
    "STAdj2=[]\n",
    "for i in STAdj11:\n",
    "    count=0\n",
    "    ST3=[]\n",
    "    for j in i:\n",
    "        if(count<=10):\n",
    "            ST3.append(j[1])\n",
    "        else:\n",
    "            break\n",
    "        count+=1\n",
    "    STAdj2.append(ST3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "STNoun2=[]\n",
    "for i in STNoun11:\n",
    "    count=0\n",
    "    ST3=[]\n",
    "    for j in i:\n",
    "        if(count<=10):\n",
    "            ST3.append(j[1])\n",
    "        else:\n",
    "            break\n",
    "        count+=1\n",
    "    STNoun2.append(ST3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting 2-D list to dataFrame\n",
    "#final result are these two csv files each under category NOUN and ADJECTIVE\n",
    "df =pd.DataFrame(STAdj2)\n",
    "df.to_csv(\"STAdj.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.DataFrame(STNoun2)\n",
    "df.to_csv(\"STNoun.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summa keywords.keywords are not performing well as evident from the first story content output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A fawn was racing in the forest He was ahead of the rabbit He was ahead of the elephant He leapt and cleared the stream He ran past the crumbling wall There was a large boulder on the grassy plain He stumbled and fell down He burst into tears The monkey massaged his leg Tears flowed from the fawns eyes Brother Bear picked him up The fawn didnt stop crying His mother came She said Look well beat up this bad boulder The fawn said Oh dont do that or he will also start crying His mother laughed So did the fawn\n",
      "fawn\n",
      "fawns eyes\n",
      "didnt\n",
      "oh dont\n",
      "bear\n",
      "['fawn', 'fawns eyes', 'didnt', 'oh dont', 'bear']\n"
     ]
    }
   ],
   "source": [
    "a1=remove_special_characters(w)\n",
    "print(a1)\n",
    "a=keywords.keywords(a1)\n",
    "print(a)\n",
    "y=[s.strip() for s in a.splitlines()]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df[\"story_content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (df[\"story_content\"][1314])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    \n",
    "    #checking that should we remove the digits as well or not\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    \n",
    "    #find the pattern in text and replace it with ''\n",
    "    text = re.sub(pattern,'', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=[]\n",
    "for item in range(len(x)):\n",
    "    if(item == 1314):\n",
    "        continue\n",
    "    a1=remove_special_characters(x[item])\n",
    "    a=keywords.keywords(a1)\n",
    "    y=[s.strip() for s in a.splitlines()]\n",
    "    x1.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_keyword(x,i,f):\n",
    "    k=[]\n",
    "    for item in x[i:f]:\n",
    "        print(\"##########1\")\n",
    "        a1=remove_special_characters(item)\n",
    "        print(a1)\n",
    "        a=keywords.keywords(a1)\n",
    "        print(\"##########2\")\n",
    "        print(a)\n",
    "        y=[s.strip() for s in a.splitlines()]\n",
    "        print(\"##########3\")\n",
    "        print(y)\n",
    "        k.append(y)\n",
    "        print(\"##########4\")\n",
    "        print(k)\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########1\n",
      "A fawn was racing in the forest He was ahead of the rabbit He was ahead of the elephant He leapt and cleared the stream He ran past the crumbling wall There was a large boulder on the grassy plain He stumbled and fell down He burst into tears The monkey massaged his leg Tears flowed from the fawns eyes Brother Bear picked him up The fawn didnt stop crying His mother came She said Look well beat up this bad boulder The fawn said Oh dont do that or he will also start crying His mother laughed So did the fawn\n",
      "##########2\n",
      "fawn\n",
      "fawns eyes\n",
      "didnt\n",
      "oh dont\n",
      "bear\n",
      "##########3\n",
      "['fawn', 'fawns eyes', 'didnt', 'oh dont', 'bear']\n",
      "##########4\n",
      "[['fawn', 'fawns eyes', 'didnt', 'oh dont', 'bear']]\n"
     ]
    }
   ],
   "source": [
    "#since this didn't work\n",
    "# from summa import commons, graph, keywords, pagerank_weighted\n",
    "# from summa import summarizer, syntactic_unit, textrank\n",
    "\n",
    "a=add_keyword(x,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fawn', 'fawns eyes', 'didnt', 'oh dont', 'bear']]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we tried this\n",
    "x1=[]\n",
    "for item in x[0:1000]:\n",
    "    a1=remove_special_characters(item)\n",
    "    a=keywords.keywords(a1)\n",
    "    y=[s.strip() for s in a.splitlines()]\n",
    "    x1.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it's definitely between 1300-1350\n",
    "#1400-1500 is safe\n",
    "#1500-1700 is safe\n",
    "#1700 to 1900 is safe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
